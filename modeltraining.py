# -*- coding: utf-8 -*-
"""ModelTraining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115RmGKCSSLsHL7xD2GzXUXS3VxByLXBC
"""

# !pip install torch --upgrade
# !pip install transformers --upgrade
# !pip install datasets

# !pip install datasets pandas

# !pip install openpyxl # Install the openpyxl library

# !pip install torch==2.0.0 transformers==4.10.0 datasets==1.11.0

# !pip install datasets

pip install pynvml

"""###Consumption statistics:"""

import time
import psutil
import subprocess
import pandas as pd
import logging
from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer, TrainerCallback, TrainerState, TrainerControl

# Function to get GPU power and utilization using nvidia-smi
def get_gpu_power_and_utilization():
    try:
        gpu_stats = subprocess.check_output(
            ["nvidia-smi", "--query-gpu=power.draw,utilization.gpu,memory.used,memory.total", "--format=csv,noheader,nounits"]
        ).decode("utf-8").strip().split("\n")
        gpu_stats = [stat.split(", ") for stat in gpu_stats][0]
        gpu_power = float(gpu_stats[0])  # Power in watts
        gpu_utilization = float(gpu_stats[1])  # Utilization in percentage
        gpu_memory_utilization = (float(gpu_stats[2]) / float(gpu_stats[3])) * 100  # Memory utilization in percentage
        return gpu_power, gpu_utilization, gpu_memory_utilization
    except Exception as e:
        logging.error(f"Error getting GPU stats: {e}")
        return 0.0, 0.0, 0.0

# Function to get power consumption
def get_power_consumption():
    gpu_power, gpu_utilization, gpu_memory_utilization = get_gpu_power_and_utilization()
    cpu_utilization = psutil.cpu_percent(interval=1)
    memory_utilization = psutil.virtual_memory().percent
    return gpu_power, gpu_utilization, gpu_memory_utilization, cpu_utilization, memory_utilization

# Custom callback to log resource usage
class ResourceLoggerCallback(TrainerCallback):
    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        gpu_power, gpu_utilization, gpu_memory_utilization, cpu_utilization, memory_utilization = get_power_consumption()
        timestamp = time.time() - start_time
        data.append([timestamp, gpu_power, gpu_utilization, gpu_memory_utilization, cpu_utilization, memory_utilization])

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

import os
import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# %cd drive/MyDrive/NICHackathon/
# !unzip training_data.zip
# !unzip test_data.zip

!pwd

directory = '/content/drive/MyDrive/NICHackathon/training_data'
csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.xlsx')]

print(csv_files)

import openpyxl
# Load and concatenate all Excel files into a single DataFrame
dataframes = [pd.read_excel(file) for file in csv_files]
data = pd.concat(dataframes, ignore_index=True)

df = data.copy()
# Check the data
print(df)

df.drop(['year', 'index'], axis=1, inplace=True)

df

!pip install datasets

from transformers import RobertaTokenizer, RobertaForSequenceClassification
from datasets import Dataset

# Convert the DataFrame to a Hugging Face Dataset
dataset = Dataset.from_pandas(data)
# Load the tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-large')

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['sentence'], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Check for NaNs in the dataset
assert data['sentence'].notna().all(), "There are NaNs in the 'sentence' column"
assert data['label'].notna().all(), "There are NaNs in the 'label' column"
assert data['label'].isin([0, 1,2]).all(), "Labels should be either 0 or 1"

from transformers import Trainer, TrainingArguments

!pip install pynvml

# Load the tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-large")
model = RobertaForSequenceClassification.from_pretrained("roberta-large", num_labels=3)

# Split the dataset into train and test sets (80-20 split in this example)
split_datasets = tokenized_datasets.train_test_split(test_size=0.2)
train_dataset = split_datasets['train']
test_dataset = split_datasets['test']

# Initialize NVML for GPU stats
try:
    pynvml.nvmlInit()
    nvml_available = True
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
except Exception as e:
    logging.warning("NVML not available. Using nvidia-smi as fallback.")
    nvml_available = False

# Prepare training arguments
training_args = TrainingArguments(
    output_dir="./results",
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="steps",
    eval_strategy="epoch",  # updated to eval_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # Reduced batch size
    per_device_eval_batch_size=16,   # Reduced batch size
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
    gradient_accumulation_steps=2,  # Gradient accumulation
)

# Data storage
data = []

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    callbacks=[ResourceLoggerCallback]
)

# Start monitoring resource usage
start_time = time.time()

# Train the model
trainer.train()

# Create a DataFrame
df = pd.DataFrame(data, columns=['Timestamp', 'GPU Power (W)', 'GPU Utilization (%)', 'GPU Memory Utilization (%)', 'CPU Utilization (%)', 'Memory Utilization (%)'])

# Calculate total energy consumption
df['GPU Energy Consumption (kWh)'] = df['GPU Power (W)'] / 3600  # Convert to kWh
df['CPU Energy Consumption (kWh)'] = df['CPU Utilization (%)'] / 3600  # Placeholder: Adjust based on actual CPU power measurement

# Calculate total cost
electricity_cost = 0.13  # Example value
total_gpu_energy = df['GPU Energy Consumption (kWh)'].sum()
total_cpu_energy = df['CPU Energy Consumption (kWh)'].sum()
total_energy_cost = (total_gpu_energy + total_cpu_energy) * electricity_cost

# Print total energy cost
print(f"Total GPU Energy Consumption: {total_gpu_energy:.4f} kWh")
print(f"Total CPU Energy Consumption: {total_cpu_energy:.4f} kWh")
print(f"Total Energy Cost: ${total_energy_cost:.4f}")

# Save DataFrame to CSV
df.to_csv('resource_consumption.csv', index=False)

# Evaluate the model
results = trainer.evaluate()
print(results)
trainer.save_model("./finetuned_roberta_large_Hawk")

# Shutdown NVML if available
if nvml_available:
    pynvml.nvmlShutdown()

import shutil

drive_path = '/content/drive/MyDrive/NICHackathon/finetuned_roberta_large_Hawk'

shutil.copytree('./finetuned_roberta_large_Hawk', drive_path)

#For the trust sentiment

directory = '/content/drive/MyDrive/NICHackathon/gemini-trust-data'
csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]

print(csv_files)

dataframes = [pd.read_csv(file) for file in csv_files]
data = pd.concat(dataframes, ignore_index=True)

df = data.copy()
# Check the data
print(df)

df = df.drop(['true_label'], axis = 1)

# Create the new 'label' column based on the conditions
# Create the new 'label' column based on the conditions
def assign_label(text):
    if 'MISTRUST' in text:
        return 2
    elif 'TRUST' in text:
        return 1
    elif 'NEUTRAL' in text:
        return 0
    else:
        return -1  # Use -1 to indicate texts that do not contain any of the specified words

df['label'] = df['text_output'].apply(assign_label)

# Display the DataFrame with the new 'label' column
df

# Filter out rows with label -1
df = df[df['label'] != -1]

df

pip install transformers[torch]

!pip install transformers[torch] accelerate -U

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Prepare the dataset for training
dataset = Dataset.from_pandas(df[['original_sent', 'label']])

# Split the dataset into train and test sets (80-20 split in this example)
split_datasets = dataset.train_test_split(test_size=0.2)
train_dataset = split_datasets['train']
test_dataset = split_datasets['test']

# Tokenize the sentences
def tokenize_function(examples):
    return tokenizer(examples['original_sent'], truncation=True, padding=True)

# Load the tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-large")
model = RobertaForSequenceClassification.from_pretrained("roberta-large", num_labels=3)

# Tokenize the datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set the format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# # Initialize NVML for GPU stats
# try:
#     pynvml.nvmlInit()
#     nvml_available = True
#     handle = pynvml.nvmlDeviceGetHandleByIndex(0)
# except Exception as e:
#     logging.warning("NVML not available. Using nvidia-smi as fallback.")
#     nvml_available = False

# Prepare training arguments
training_args = TrainingArguments(
    output_dir="./results2",
    logging_dir='./logs2',
    logging_steps=10,
    evaluation_strategy="epoch",
    # eval_strategy="epoch",  # updated to eval_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # Reduced batch size
    per_device_eval_batch_size=16,   # Reduced batch size
    num_train_epochs=10,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
    gradient_accumulation_steps=2,  # Gradient accumulation
)

# Data storage
data = []

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    callbacks=[ResourceLoggerCallback]
)

# Start monitoring resource usage
start_time = time.time()

# Train the model
trainer.train()

# Create a DataFrame for resource consumption
df_stats = pd.DataFrame(data, columns=['Timestamp', 'GPU Power (W)', 'GPU Utilization (%)', 'GPU Memory Utilization (%)', 'CPU Utilization (%)', 'Memory Utilization (%)'])

# Calculate total energy consumption
df_stats['GPU Energy Consumption (kWh)'] = df_stats['GPU Power (W)'] / 3600  # Convert to kWh
df_stats['CPU Energy Consumption (kWh)'] = df_stats['CPU Utilization (%)'] / 3600  # Placeholder: Adjust based on actual CPU power measurement

# Calculate total cost
electricity_cost = 0.13  # Example value
total_gpu_energy = df_stats['GPU Energy Consumption (kWh)'].sum()
total_cpu_energy = df_stats['CPU Energy Consumption (kWh)'].sum()
total_energy_cost = (total_gpu_energy + total_cpu_energy) * electricity_cost

# Print total energy cost
print(f"Total GPU Energy Consumption: {total_gpu_energy:.4f} kWh")
print(f"Total CPU Energy Consumption: {total_cpu_energy:.4f} kWh")
print(f"Total Energy Cost: ${total_energy_cost:.4f}")

# Save DataFrame to CSV
df_stats.to_csv('resource_consumption2.csv', index=False)

# Evaluate the model
results = trainer.evaluate()
print(results)
trainer.save_model("./finetuned_roberta_large_trust")

# Save the model to Google Drive
drive_path = '/content/drive/MyDrive/NICHackathon/finetuned_roberta_large_trust_2'
shutil.copytree('./finetuned_roberta_large_trust', drive_path)

# Save the model to Google Drive
drive_path = '/content/drive/MyDrive/NICHackathon/finetuned_roberta_large_trust_2'
shutil.copytree('./finetuned_roberta_large_trust', drive_path)

# # Save the model to Google Drive
# drive_path = '/content/drive/MyDrive/NICHackathon/finetuned_roberta_large_trust_2'
# shutil.copytree('./finetuned_roberta_large_trust', drive_path)

#For the stability sentiment

directory = '/content/drive/MyDrive/NICHackathon/gemini-stability-data'
csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]

print(csv_files)

dataframes = [pd.read_csv(file) for file in csv_files]
data = pd.concat(dataframes, ignore_index=True)

df = data.copy()
# Check the data
print(df)

df = df.drop(['true_label'], axis = 1)

def assign_label(text):
    if 'VOLATILE' in text:
        return 2
    elif 'STABLE' in text:
        return 1
    elif 'NO-EFFECT' in text:
        return 0
    else:
        return -1  # Use -1 to indicate texts that do not contain any of the specified words

df['label'] = df['text_output'].apply(assign_label)

# Display the DataFrame with the new 'label' column
df

# Filter out rows with label -1
df = df[df['label'] != -1]

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Prepare the dataset for training
dataset = Dataset.from_pandas(df[['original_sent', 'label']])

# Split the dataset into train and test sets (80-20 split in this example)
split_datasets = dataset.train_test_split(test_size=0.2)
train_dataset = split_datasets['train']
test_dataset = split_datasets['test']

# Tokenize the sentences
def tokenize_function(examples):
    return tokenizer(examples['original_sent'], truncation=True, padding=True)

# Load the tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-large")
model = RobertaForSequenceClassification.from_pretrained("roberta-large", num_labels=3)

# Tokenize the datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set the format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# # Initialize NVML for GPU stats
# try:
#     pynvml.nvmlInit()
#     nvml_available = True
#     handle = pynvml.nvmlDeviceGetHandleByIndex(0)
# except Exception as e:
#     logging.warning("NVML not available. Using nvidia-smi as fallback.")
#     nvml_available = False

# Prepare training arguments
training_args = TrainingArguments(
    output_dir="./results3",
    logging_dir='./logs3',
    logging_steps=10,
    evaluation_strategy="epoch",
    # eval_strategy="epoch",  # updated to eval_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # Reduced batch size
    per_device_eval_batch_size=16,   # Reduced batch size
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
    gradient_accumulation_steps=2,  # Gradient accumulation
)

# Data storage
data = []

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    callbacks=[ResourceLoggerCallback]
)

# Start monitoring resource usage
start_time = time.time()

# Train the model
trainer.train()

# Create a DataFrame for resource consumption
df_stats = pd.DataFrame(data, columns=['Timestamp', 'GPU Power (W)', 'GPU Utilization (%)', 'GPU Memory Utilization (%)', 'CPU Utilization (%)', 'Memory Utilization (%)'])

# Calculate total energy consumption
df_stats['GPU Energy Consumption (kWh)'] = df_stats['GPU Power (W)'] / 3600  # Convert to kWh
df_stats['CPU Energy Consumption (kWh)'] = df_stats['CPU Utilization (%)'] / 3600  # Placeholder: Adjust based on actual CPU power measurement

# Calculate total cost
electricity_cost = 0.13  # Example value
total_gpu_energy = df_stats['GPU Energy Consumption (kWh)'].sum()
total_cpu_energy = df_stats['CPU Energy Consumption (kWh)'].sum()
total_energy_cost = (total_gpu_energy + total_cpu_energy) * electricity_cost

# Print total energy cost
print(f"Total GPU Energy Consumption: {total_gpu_energy:.4f} kWh")
print(f"Total CPU Energy Consumption: {total_cpu_energy:.4f} kWh")
print(f"Total Energy Cost: ${total_energy_cost:.4f}")

# Save DataFrame to CSV
df_stats.to_csv('resource_consumption3.csv', index=False)

# Evaluate the model
results = trainer.evaluate()
print(results)
trainer.save_model("./finetuned_roberta_large_stability")

# Save the model to Google Drive
drive_path = '/content/drive/MyDrive/NICHackathon/finetuned_roberta_large_stability_'
shutil.copytree('./finetuned_roberta_large_stability', drive_path)

